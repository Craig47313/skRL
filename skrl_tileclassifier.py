''' -*- coding: utf-8 -*-
learned from Rob Mulla's 'Train Your first PyTorch Model [Card Classifier]' which can be found at https://www.kaggle.com/code/robikscube/train-your-first-pytorch-model-card-classifier
--> also learned using Kie Codes 'Image Classifier in PyTorch' which can be found at https://www.youtube.com/watch?v=igQeI29FIQM
will create smth better using a pretained model like resnet and increase epochs 
this code is not for production and is was made for a colab notebook @ https://colab.research.google.com/drive/1I3KSzAExvrsQhLlgN-Klkl03bW5RO6ud?authuser=0#scrollTo=ubocnol43kSh
with about data balenced at 25~ images per class, this achieved Accuracy:  0.8812 Precision: 0.9075 Recall: 0.8812 F1 Score: 0.8840
some code modified from original by chatgpt such as printing recall, precision, and f1 instead of just accuracy'''

"""skRL_TileClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I3KSzAExvrsQhLlgN-Klkl03bW5RO6ud
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchsummary import summary
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import random_split
from torch.utils.data import DataLoader

device = torch.device("cpu")
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_built() and torch.backends.mps.is_available():
    device = torch.device("mps")
print(device)

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        (0.5, 0.5, 0.5),
        (0.5, 0.5, 0.5)
    )
])

train_set = torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/dataBalenced/test", transform=transform)
test_set = torchvision.datasets.ImageFolder(root="/content/drive/MyDrive/dataBalenced/train", transform=transform)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=True)

classes = ("bishop", "board", "king", "knight", "pawn", "player", "queen", "rook")

print(f"Shape of the images in the training dataset: {train_loader.dataset[0][0].shape}")

fig, axes = plt.subplots(1, 10, figsize=(12, 3))
for i in range(10):
    image = train_loader.dataset[i][0].permute(1, 2, 0)
    denormalized_image= image / 2 + 0.5
    axes[i].imshow(denormalized_image)
    axes[i].set_title(classes[train_loader.dataset[i][1]])
    axes[i].axis('off')
plt.show()

class ConvNeuralNet(nn.Module):
    def __init__(self, num_classes=8):  # change num_classes if not 10
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3)
        self.conv2 = nn.Conv2d(64, 128, 3)
        self.pool = nn.MaxPool2d(2, stride=2)

        # match flattened size after convs + pools
        self.fc1 = nn.Linear(128 * 35 * 35, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.log_softmax(self.fc3(x), dim=1)
        return x
net = ConvNeuralNet()
net.to(device)

loss_function = nn.NLLLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

epochs = 10
for epoch in range(epochs):

    running_loss = 0.0
    for i, data in enumerate(train_loader):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()
        outputs = net(inputs)
        loss = loss_function(outputs, labels)

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 20 == 19:
            print(f'[{epoch + 1}/{epochs}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

print('Finished Training')

def view_classification(image, probabilities):
    probabilities = probabilities.data.numpy().squeeze()

    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)

    image = image.permute(1, 2, 0)
    denormalized_image= image / 2 + 0.5
    ax1.imshow(denormalized_image)
    ax1.axis('off')

    ax2.barh(np.arange(len(classes)), probabilities)
    ax2.set_aspect(0.1)
    ax2.set_yticks(np.arange(len(classes)))
    ax2.set_yticklabels(classes)
    ax2.set_title('Class Probability')
    ax2.set_xlim(0, 1.1)
    plt.tight_layout()

images, _ = next(iter(test_loader))

image = images[3]
batched_image = image.unsqueeze(0).to(device)
with torch.no_grad():
    log_probabilities = net(batched_image)

probabilities = torch.exp(log_probabilities).squeeze().cpu()
view_classification(image, probabilities)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

net.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

acc = accuracy_score(all_labels, all_preds)
prec = precision_score(all_labels, all_preds, average="macro", zero_division=0)
rec = recall_score(all_labels, all_preds, average="macro", zero_division=0)
f1 = f1_score(all_labels, all_preds, average="macro", zero_division=0)

print(f"Test Metrics:\n"
      f"Accuracy:  {acc:.4f}\n"
      f"Precision: {prec:.4f}\n"
      f"Recall:    {rec:.4f}\n"
      f"F1 Score:  {f1:.4f}")